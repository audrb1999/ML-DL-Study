{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Andrew_Ng_2_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPNWfVfwyvHkoPp8eu9H8DE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/audrb1999/ML-DL-Study/blob/main/%EC%8B%AC%EC%B8%B5%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%84%B1%EB%8A%A5%20%ED%96%A5%EC%83%81%EC%8B%9C%ED%82%A4%EA%B8%B0/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EC%96%B4%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98%20%EC%84%A4%EC%A0%95%ED%95%98%EA%B8%B0/04.%EC%A0%95%EA%B7%9C%ED%99%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L50iCb89n8qq"
      },
      "source": [
        "#***정규화 (Regularization)***\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eR1vemCoGOm"
      },
      "source": [
        "### 높은 분산으로 Overfitting이 의심간다면?\n",
        "> 가장 처음 시도해야 할 것은 정규화 \n",
        "\n",
        "> 또는 더 많은 훈련 데이터를 얻는 것 > 하지만 비용이 듬"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irZJWXtdq71S"
      },
      "source": [
        "###정규화 작동 방식"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6mXl6IQrByC"
      },
      "source": [
        "> J = 비용 함수 아래식은 비용함수를 최소화 시키는 것 \n",
        "\n",
        "$J(w,b)$ = Σ$\\frac1 M$ $L$(y^, y) # 아직 미완성본\n",
        "\n",
        "min J (w, b) \n",
        "\n",
        "w ∈ $R^n$ , b ∈ $R$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG5gPGAzrBqz"
      },
      "source": [
        "- 로지스틱 회귀에 정규화를 추가하려면 정규화 매개변수라고 불리는 $λ$ 사용해야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A93nTLyrBny"
      },
      "source": [
        "$J(w,b)$ = Σ$\\frac1 M$ $L$(y^, y) + $\\frac \\lambda 2m$$||w^2 2||$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9-nWM_Lsqca"
      },
      "source": [
        "여기서 w 제곱의 노름 $||w^2 2||$은  j의 1부터 n(x의 차원)까지 $wj^2$값을 더한 것과 같습니다\n",
        "\n",
        "1. L2 정규화 = 벡터 w의 유클리드 노름\n",
        "> $||w^2 2||$  = Σ$w^2j$ = $w^T * w$\n",
        "- 매개변수 벡터 w의 유클리드 노름의 제곱\n",
        "- 입력 데이터는 고정(단위벡터), 가중치 w값에 따라 바뀌는 그걸 제곱하면 유클리드 노름이 나오는 것?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAVr9GKXxS1C"
      },
      "source": [
        "###b에 대해서는 정규화를 안하는 이유\n",
        "\n",
        ">b도 해줄 수는 있지만 보통 생략, w는 높은 차원의 매개변수 벡터이기 때문에 해야함 > 특히 높은 분산을 가질 때 많은 매개변수를 가짐\n",
        "\n",
        ">반면 b는 하나의 숫자 > 모든 매개변수는 실질적으로 w에 있음 이 마지막항 b를 넣어도 실질적이 차이가 없습니다.\n",
        "- 그 많은 매개변수중 하나가 b이기 때문에 미미하다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avgZ_hFjx_iC"
      },
      "source": [
        "###2. L1 정규화 \n",
        "\n",
        "> $\\frac \\lambda 2M$ Σ$|wj|$ = $\\fracλ 2M ||w1||$    <j는 1~n까지>\n",
        "- L1 노름 이용이 w가 희소해지는데 이는 w벡터 안에 0이 많아진다는 의미입니다 \n",
        ">> 어떤 사람들은 이것이 모델이 압축하는데 도움이 된다고 합니다.\n",
        "왜냐하면 특정 매개변수가 0일 경우 메모리가 적게 필요하기 때문입니다.\n",
        "\n",
        "그러나 모델을 희소하게 만들기 위해 L1 정규화를 사용하는 것은 큰 도움이 되지 않습니다. \n",
        "\n",
        "사람들이 네트워크를 훈련할 때에는 L2 정규화를 훨씬 더 많이 사용합니다.\n",
        "\n",
        "\n",
        "$\\lambda$ 는 정규화 매개변수라고 불립니다. \n",
        "다양한 값을 시도해서 훈련 세트에 잘 맞으면서 두 매개변수의 노름을 잘 설정해 overfitting을 막을 수 있는 최적의 값을 찾습니다.\n",
        "\n",
        "즉 람다도 하이퍼피라매터입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63kKhldBaw3L"
      },
      "source": [
        "손실함수는 모든 층의 오차율을 평균을 낸 것!  > 그 값을 위에서부터 흘려줘서 체인룰을 적용한 것!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptaU1tTMfmfF"
      },
      "source": [
        "### 신경망에서\n",
        "\n",
        "##$J(w,b)$ = Σ$\\frac1 M$ $L$(y^, y) +  $\\frac λ 2m$Σ$||w^i||^2$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ideM9jABf6V1"
      },
      "source": [
        "$||w^i||^2$  는 행렬의 노름!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca2VpzougF1c"
      },
      "source": [
        "\n",
        "###$||w^i||^2$ = $ΣΣ ||w^l ij||^2$  이 값은 행렬 제곱의 합이라는 의미입니다.\n",
        "첫 번째는 i에 대한 시그마 두 번째는 j에 대한 시그마\n",
        "\n",
        "이 행렬의 노름은 프로베니우스 노름이라고 부릅니다.\n",
        "\n",
        "행렬의 L2 노름이라고 부르는 대신에 프로베니우스 노름이라고 부릅니다.\n",
        "\n",
        "즉 L2 노름은 행렬 원소 제곱의 합으로 구할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjOQyEE5hhEe"
      },
      "source": [
        "이것을 역전파로 구하는 방법\n",
        "\n",
        "- 이전에는 기울기를 구해 해당 가중치에서 빼줬습니다.\n",
        "- $w - α dw$ 이것은 추가적인 정규항을 더해주기 이전의 값\n",
        "- 즉 이것을 적용한다면 dw = dw +  $\\frac \\lambda M ||w1||$ \n",
        "- 2로 나누는 건 스케일링 값"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFUVXB_ZkOXu"
      },
      "source": [
        "이것은 단순히 기울기 뒤에 더해서 오차역전파 때 가중치값을 줄이는 것!\n",
        "\n",
        "dw = dw +  $\\frac\\lambda M ||w1||$  이렇게 더해서 빼니까 L2 노름값이 작다해도 w값은 이전보다 작아짐\n",
        "\n",
        "그래서 이것을 가중치 감쇠라고 부름!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxzjQS9sk0HN"
      },
      "source": [
        "L2 노름 정규화가 가중치 감쇠라고 불리는 이유!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qw5Bcp6k769"
      },
      "source": [
        "#$w^l$ = ( 1 − $\\fracαm$ $\\lambda$) $||w^l||$  −α  (역전파에서 온 값들)"
      ]
    }
  ]
}