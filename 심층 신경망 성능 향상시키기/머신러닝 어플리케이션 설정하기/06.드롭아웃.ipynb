{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06.드롭아웃.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOkrOPuwmneUfG4QLxv/JMV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/audrb1999/ML-DL-Study/blob/main/%EC%8B%AC%EC%B8%B5%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%84%B1%EB%8A%A5%20%ED%96%A5%EC%83%81%EC%8B%9C%ED%82%A4%EA%B8%B0/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EC%96%B4%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98%20%EC%84%A4%EC%A0%95%ED%95%98%EA%B8%B0/06.%EB%93%9C%EB%A1%AD%EC%95%84%EC%9B%83.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx9YBdNY6Jmp"
      },
      "source": [
        "#***정규화 기법 2: 드롭아웃***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z0NOse_6JkB"
      },
      "source": [
        "드롭아웃의 기법은 각 노드별 삭제 확률을 설정하는 것!\n",
        "\n",
        "그러면 그 노드의 입출력이 전부 삭제됨 > 최종 출력의 영향을 못 줌\n",
        "\n",
        "순전파 역전파 모두에서 영향을 못 주게 됨!\n",
        "\n",
        "어느 부분에서 어떤 애들이 과하게 학습되는지 모르니까 랜덤으로 영향을 0으로 만들어버림"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j3d3A3a6JgC"
      },
      "source": [
        "> 이것은 랜덤 포레스트 트리처럼 다양한 네트워크를 훈련시켜 오버피팅을 막는 방법과 유사\n",
        "\n",
        "> 각각의 샘플에서 더 작은 네트워크 여러 개를 훈련시켜 한쪽에 치중되지 않게 하는 것"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Logmp9_56Jdp"
      },
      "source": [
        "##드롭아웃을 구현하는 방법\n",
        "1. 역 드롭아웃\n",
        "\n",
        "L = 3인 3층 신경망을 예로들면"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhE27NHB6JaB"
      },
      "source": [
        "먼저 3층에 대한 드롭아웃 벡터인 d3 설정 \n",
        "\n",
        "> d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob\n",
        "\n",
        "활성화 함수까지 연산된 그 층의 출력과 형상을 맞추고 그 값이 keep_prob이라는 숫자보다 작은지 비교 keep_prob은 노드가 유지될 확률! keep_prob이 0.8이면 삭제확률이 0.2라는 것\n",
        "\n",
        "> a3 = np.matmul(a3, d3)  \n",
        "  각 행렬의 원소끼리 곱을 진행하는 것\n",
        "\n",
        "모든 원소(픽쳐 데이터)에 대해서 20퍼센트의 확률로 0이 되는 d3의 원소를 곱하는 것\n",
        "\n",
        "> a3 /= keep_prob  - 역 드롭아웃 기법   \n",
        "  a3 = a3/keep_prob 와 같은 맥락\n",
        "\n",
        "세 번째 은닉층에 50개의 유닛 즉 50개의 뉴런이 있다고 하면 0.2의 확률로 삭제된다고 하면 평균적으로 10개의 유닛이 삭제된다는 의미입니다.\n",
        "\n",
        "> z4 = w4 * a3 + b4   \n",
        "  이면 a3의 원소 20퍼센트가 0이 된다는 의미입니다.  \n",
        "\n",
        "그러면 z4의 기대값을 줄이지 않기 위해 a3값을 0.8로 나눠줘야합니다.\n",
        "왜냐하면 필요한 20퍼센트 정도의 값을 다시 원래대로 만들 수 있기 때문입니다.\n",
        "이를 통해 a3의 기대값을 유지할 수 있습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT7uKoFq6JWq"
      },
      "source": [
        "keep_prob으로 다시 나눠줌으로써 a3의 기대값을 같게 유지합니다.\n",
        "\n",
        "(50 - 10) / 0.8 = 50 이 나옴"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FksInaWsDYal"
      },
      "source": [
        "이후 여러번 반복시키면 됨!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw2ims9ADaE0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}