{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11.심층 신경망의 가중치 초기화.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMNUqGvz/xiJIFSEecFYryU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/audrb1999/ML-DL-Study/blob/main/%EC%8B%AC%EC%B8%B5%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%84%B1%EB%8A%A5%20%ED%96%A5%EC%83%81%EC%8B%9C%ED%82%A4%EA%B8%B0/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EC%96%B4%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98%20%EC%84%A4%EC%A0%95%ED%95%98%EA%B8%B0/11.%EC%8B%AC%EC%B8%B5_%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98_%EA%B0%80%EC%A4%91%EC%B9%98_%EC%B4%88%EA%B8%B0%ED%99%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8X6QUyI4XYL"
      },
      "source": [
        "#  심층 신경망의 가중치 초기화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKvfullj4XV8"
      },
      "source": [
        "매우 깊은 신경망을 훈련시킬 때, 미분값 혹은 기울기가 아주 작아지거나 커지는 문제를 해결하기 위한 방법\n",
        "\n",
        "1. 신경망에 대한 무작위 초기화를 좀 더 신중하게 선택한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pxzbhbug4XTL"
      },
      "source": [
        "분산을 $\\frac 1 n$로 설정하는 방법 n은l의 뉴런으로 들어가는 입력 특성의 개수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIIG6Z5e4XQb"
      },
      "source": [
        "$w^{[1]}$ = np.random.randn(shape) * np.sqrt($\\frac 1 {n^{l -1}} $)\n",
        "\n",
        "- l = 해당 레이어 층을 의미 1층 2층..\n",
        "\n",
        "ReLU에 경우에는 분산을 $\\frac2 n$ 으로 설정하는게 더 잘 작동합니다.\n",
        "\n",
        "가중치 값이 작아짐 > 가중치 값과 입력값을 곱했을 때 그 활성화 값이 0에 분포할 확률이 높아짐 > 0에 가까울수록 기울기가 안 없어짐"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0swWQl44XK1"
      },
      "source": [
        "즉 w값을 1보다 너무 크지않게 작지 않게 설정해서 너무 빨리 폭발하거나 소실되지 않게 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrRMacSV4XIM"
      },
      "source": [
        "## tanh를 활성화 함수로 사용한다면\n",
        "\n",
        "상수 2 대신 1을 사용해야함 $\\frac 1 {n^{l -1}} $\n",
        "\n",
        "- 또 다른 방법 세이비어 초기화\n",
        "\n",
        "$\\frac 2 {n^{l -1} + n^l} $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBqfqCty4XFs"
      },
      "source": [
        "이 모든 식들은 그저 시작점을 제공!\n",
        "\n",
        "가중치 행렬의 초기 분산에 대한 기본 값을 줄 뿐\n",
        "\n",
        "이와 같은 분산을 원한다면 분산 매개변수는 하이퍼파라미터로 조정할 또 다른 값이 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYBqCabU7tL2"
      },
      "source": [
        "가중치 소실은 시그모이드나 tanh나 ReLU에서 마이너스로 가는 것이고\n",
        "가중치 폭발은 양수로 가는 것인가??"
      ]
    }
  ]
}